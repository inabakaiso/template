data:
  train_data_path: "../input/train.csv"
  test_data_path: "../input/train.csv"
  train_txt_path: "../input/2022/train"
  test_txt_path: "../input/test"
  output_dir_path: "output"
  input_dir_path: "../input"
cv_strategy:
  seed: 42
  num_split: 4
  shuffle: true
  split_type: "multi_label_skfold"
model:
  model_name: "microsoft/deberta-v2-xlarge"
tokenizer:
  tokenizer_name: ${model.model_name}
dataset:
  batch_size: 4
  num_workers: 4
  max_len: 512
  train: true
  target_cols: ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']
  input_col: 'full_text'
scheduler:
  scheduler_name: "cosine"
  num_warmup_steps: 0
  num_cycles: 0.5
optimizer:
  opt_mode: 'specific'
  learning_rate: 0.00002
training:
  dropout: 0.2
  debug: false
  batch_scheduler: true
  gradient_checkpoint: true
  apex: true
  epochs: 4
  batch_size: ${dataset.batch_size}
  weight_decay: 0.01
  max_grad_norm: 1000
  eps: 0.000001
  min_lr: 0.0000001
  t_0: 500
  encoder_lr: 0.00002
  decoder_lr: 0.00002
  print_freq: 20
  gradient_accumulation_steps: 1
  accumulation_steps: 1
  gpu_optimize_config:
    fp16: true
    freezing: true
    optim8bit: true
    gradient_checkpoint: true